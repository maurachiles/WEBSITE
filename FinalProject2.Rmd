---
title: "Employee Satisfaction with Gradient Boosting"
author: "Maura Chiles"
date: "`r Sys.Date()`"
output: distill::distill_article
---

```{r, echo=FALSE}
knitr::opts_chunk$set(error = TRUE)
```

# Predicting Employee Satisfaction with a Gradient Boosting Model

## Description of Data

This is data from a survey of 500 employees on whether they are satisfied in their position or not that I retrieved from Kaggle. The data contains variables of age, department, location, education, how they were recruited, awards they have received, and salary. I am going to predict satisfaction using gradient boosting.

```{r}
EmpSatisfaction<-read.csv("EmpSatisfaction.csv")
```

```{r, stats, echo=TRUE}
psych::describe(EmpSatisfaction)
```

Here are the descriptive statistics of the data. There are 500 observation. The dataset seems to have reasonable values and no outliers.

## Gradient Boosting

Gradient Boosting is a machine learning model that quickly learns from weak spots in its previous fittings of the data. Boosting alone is the process of initially fitting a model to the dataset (like a decision tree), then creating a second model that identifies the weak learners from the first model and improves the accuracy of the weak predictions. This process is repeated many times to improve the model. Gradient boosting uses this technique to try and create the potentially best next model combined with the previous models to decrease the overall prediction error. The name gradient comes from target outcomes being used in each model based on the gradient of the error regarding the prediction, and every new model tries to minimize the prediction error.


## Let's try this on the Dataset

We are trying to predict employee satisfaction. We are going to grab the data and split the data 60/40. 

```{r}
set.seed(1)
library(tidyverse)
library(caret)
trainIndex <- createDataPartition(EmpSatisfaction$satisfied, p = .6, list = FALSE, times = 1)
```

```{r}
ESTrain <- EmpSatisfaction[ trainIndex,]
ESTest  <- EmpSatisfaction[-trainIndex,]
```

### The Model

```{r}
set.seed(1)

ESgbm<- train(
  form = satisfied ~ age+Dept+location+education+recruitment_type+awards+salary,
  data = ESTrain,
  trControl = trainControl(method = "cv", number=10),
  method = "gbm",
  tuneLength = 20,
  verbose=FALSE)

knitr::kable(ESgbm$bestTune)
```

```{r}
ESgbm_Pred<-predict(ESgbm,ESTest,type="prob")

knitr::kable(ESgbm_Pred)%>%
  kableExtra::kable_styling("striped")%>%
  kableExtra::scroll_box(width = "50%",height="300px")
```

```{r}
ESgbmtestpred<-cbind(ESgbm_Pred,ESTest)

ESgbmtestpred<-ESgbmtestpred%>%
  mutate(prediction=if_else(yes>no,"yes",
                            if_else(no>yes, "no", "PROBLEM")))

table(ESgbmtestpred$prediction)
```


```{r}
ESgbmConfusion<-confusionMatrix(factor(ESgbmtestpred$prediction),factor(ESgbmtestpred$satisfied))

ESgbmConfusion
```

We can see here the accuracy of our model was only about 55%.

```{r}
library(gbm)
V<-caret::varImp(ESgbm, n.trees=500)$importance%>%
  arrange(desc(Overall))

knitr::kable(V)%>%
  kableExtra::kable_styling("striped")%>%
  kableExtra::scroll_box(width = "50%",height="300px")
```

```{r}
ggplot2::ggplot(V, aes(x=reorder(rownames(V),Overall), y=Overall)) +
geom_point( color="blue", size=4, alpha=0.6)+
geom_segment( aes(x=rownames(V), xend=rownames(V), y=0, yend=Overall), 
color='skyblue') +
xlab('Variable')+
ylab('Overall Importance')+
theme_light() +
coord_flip() 
```


This table and graph show which variables have the most impact on employee satisfaction in our model. Age is shown as having the highest impact, and in being in the sales department is shown as having the lowest impact.

While this model does not perform the best, machine learning could be useful in assessing employee satisfaction. Another model could be used to better evaluate what variables contribute to greater satisfaction among employees. This can be useful to firms who want to boost their employees' satisfaction with the organization in order to retain talent and foster a positive environment for employees.


Further reading on Gradient Boosting:

https://www.displayr.com/gradient-boosting-the-coolest-kid-on-the-machine-learning-block/

